# MABSearch-Learning-the-learning-rate

MABSearch is an elementary/ easy to use, gradient descent based Global Optimization algorithm. It uses RL to learn the optimal learning rate for the given objective function.

MABSearch: The Bandit Way of Learning the Learning Rate - A Harmony Between Reinforcement Learning and Gradient Descent
Published in: National Academy Science Letters Journal, Springer Publication [SCI Indexed].
Link to paper: https://link.springer.com/article/10.1007/s40009-023-01292-1

PDF of the full paper available at: https://rdcu.be/ddJ8n

What is Optimization (Video Explanation): https://www.youtube.com/watch?v=Gu7si5T0z_w

How to Cite:
Syed Shahul Hameed, A.S., Rajagopalan, N. MABSearch: The Bandit Way of Learning the Learning Rateâ€”A Harmony Between Reinforcement Learning and Gradient Descent. Natl. Acad. Sci. Lett. (2023). https://doi.org/10.1007/s40009-023-01292-1

How to Use:
There are two ipython jupyter notebook in this repository. 
0. No special prerequisite packages are required. The notebook can be downloaded and executed or the code can be simply copied.
1. An experiment-ready version titled as: "MABSearch (Experiment Ready Version).ipynb". This note book has all the GD and the proposed MABSearch algroithm.
2. An easier-to-understand version titled as: "MABSearch.ipynb", with comments explaining the proposed MABSearch algorithm alone. 

For Any suggestions or doubt mail to: shahulshan81@gmail.com
Cite the paper, if you find it useful.
